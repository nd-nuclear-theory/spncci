/****************************************************************
  spncci.cpp

  Tests of explicit SpNCCI basis construction in LSU3Shell basis.

  This code just tests normalization, but using clean refactored
  infrastructure.  Other deeper tests (of unit tensor matrix elements)
  were carried out in compute_unit_tensor_rmes.cpp.

  Required data:

  * Relative operator lsu3shell rme input files are generated by

      generate_lsu3shell_relative_operators.cpp

    which is invoked through scripting in

      compute_relative_tensors_lsu3shell_rmes.py

    Example: Z=3 N=3 twice_Nsigma0=22 Nmax=2 Nstep=2 N1v[=N1b]=1

    % python3 script/compute_relative_tensors_lsu3shell_rmes.py 3 3 22 2 2 1

    Only need .rme and .dat files.

    Not saved to repository since ~3.5 Mb...

       data/lsu3shell/lsu3shell_rme_6Li_Nmax02

       * Relative Hamiltonian (and observable) upcoupled rme files are
    generated by

      generate_relative_u3st_operators

    which is invoked manually for now as

      generate_relative_u3st_operators A Nmax N1v basename

   Example:

       ../operators/generate_relative_u3st_operators 6 2 1 hamiltonian

       with hamiltonian.load containing

       20    // hw
       Tintr 1.0    // coef
       INT 1.0 4 0 0 0 relative_observables/JISP16_Nmax20_hw20.0_rel.dat // coef Jmax J0 T0 g0 interaction_filename

       ../operators/generate_relative_u3st_operators 6 2 1 Nintr

       with Nintr.load containing

       20    // hw
       Nintr 1.0    // coef

       ../operators/generate_relative_u3st_operators 6 2 1 r2intr

       with r2intr.load containing

       20    // hw
       r2intr 1.0    // coef

   % ln -s ../../data/relative_observables/




  Anna E. McCoy and Mark A. Caprio
  University of Notre Dame

  2/20/17 (mac): Created (starting from explicit.cpp).
  4/9/17 (aem): Incorporated baby spncci hypersectors
  6/5/17 (mac): Read relative rather than intrinsic symplectic operators.
  6/16/17 (aem) : offload to computation and io control
  10/4/17 (aem) : Fixed basis construction and recurrence for A<6
  1/16/18 (aem) : Offloaded explicit construction and recurrence
    checks to explicit_construction.h
  1/30/18 (aem): Overhalled seed generation and recurrence
  2/5/18 (aem): Switched from using u3s sectors to u3s hypersectors
    combined with observable spaces
  2/15/18 (aem) : Removed gamma_max=0 lgi
    + Cleaned up codes and factored spncci.cpp into simpler functions
  5/2/19 (aem) : Moved ComputeManyBodyRMEs into hyperblocks_u3s
  6/21/19 (aem) : Extracted variance calculation functions into variance.{h,cpp}
  11/5/19 (aem) : Stripped out truncation tests and bundled initialization

Notes:
branching2 currently used for branching.  branching has old definitions still temp used for 
basis statistics
****************************************************************/

#include <cstdio>
#include <ctime>
#include <fstream>
#include <istream>
#include <iostream>
#include <sys/resource.h>
#include <omp.h>

#include "Spectra/SymEigsSolver.h"  // from spectra
#include "am/halfint.h"
#include "am/halfint_fmt.h"
#include "fmt/format.h"
#include "mcutils/parsing.h"
#include "mcutils/io.h"
#include "spncci/recurrence.h"
#include "lgi/lgi_unit_tensors.h"
#include "mcutils/profiling.h"
#include "mcutils/eigen.h"
#include "spncci/computation_control.h"
#include "spncci/decomposition.h"
#include "spncci/eigenproblem.h"
#include "spncci/explicit_construction.h"
#include "spncci/io_control.h"
#include "spncci/computation_control.h"
#include "spncci/results_output.h"
#include "spncci/transform_basis.h"
#include "spncci/vcs_cache.h"
#include "spncci/hyperblocks_u3s.h"
#include "spncci/variance.h"
#include "spncci/parameters.h"
////////////////////////////////////////////////////////////////
// WIP code
//
// to extract to spncci library when ready
////////////////////////////////////////////////////////////////
namespace spncci
{}// end namespace

////////////////////////////////////////////////////////////////
// main body
////////////////////////////////////////////////////////////////

int main(int argc, char **argv)
{
  std::cout<<"entering spncci"<<std::endl;
  ////////////////////////////////////////////////////////////////
  // initialization
  ////////////////////////////////////////////////////////////////
  //Initializes extern variables and calls Eigen::initParallel() and u3::U3CoefInit()
  spncci::InitializeSpNCCI();
  
  std::cout << "Reading control file..." << std::endl;
  spncci::RunParameters run_parameters;
  std::cout<<"Nmax="<<run_parameters.Nmax<<std::endl;
  
  ///////////////////////////////////////////////////////////////////////////////////////
  //// set up SpNCCI spaces
  ///////////////////////////////////////////////////////////////////////////////////////
  lgi::MultiplicityTaggedLGIVector lgi_families;
  spncci::SpNCCISpace spncci_space;
  spncci::SigmaIrrepMap sigma_irrep_map;
  spncci::BabySpNCCISpace baby_spncci_space;
  spncci::SpaceSpU3S spu3s_space; //Not used 
  spncci::SpaceSpLS spls_space; //Not used
  spncci::SpaceSpJ spj_space;  //Only used in obselete code
  std::vector<spncci::SpaceSpBasis> spaces_spbasis;
  spncci::KMatrixCache k_matrix_cache, kinv_matrix_cache;

  //Read in lgi families and generate spaces at different branching levels
  //Nlimit allows for different irreps to be truncated to different Nmax
  //For now just set to Nmax for all irreps 
  int Nlimit=run_parameters.Nmax;
  spncci::SetUpSpNCCISpaces(
      run_parameters,lgi_families,spncci_space,sigma_irrep_map,
      baby_spncci_space,spu3s_space,spls_space,spj_space,
      spaces_spbasis,k_matrix_cache, kinv_matrix_cache,
      Nlimit
    );

  ///////////////////////////////////////////////////////////////////////////////////////////////////
  // Results file: Writing parameters and basis statistics
  ///////////////////////////////////////////////////////////////////////////////////////////////////

  // open output files
  std::ofstream results_stream("spncci.res");

  // results output: code information
  spncci::StartNewSection(results_stream,"CODE");
  spncci::WriteCodeInformation(results_stream,run_parameters);

  // results output: run parameters
  spncci::StartNewSection(results_stream,"PARAMETERS");
  spncci::WriteRunParameters(results_stream,run_parameters);

  // results output: basis information
  spncci::StartNewSection(results_stream,"BASIS");
  spncci::WriteBasisStatistics(results_stream,spncci_space,baby_spncci_space,spu3s_space,spls_space,spj_space);
  spncci::WriteSpU3SSubspaceListing(results_stream,baby_spncci_space,run_parameters.Nsigma0);
  spncci::WriteBabySpNCCISubspaceListing(results_stream,baby_spncci_space,run_parameters.Nsigma0);


  ///////////////////////////////////////////////////////////////////////////////////////////////////
  // Enumerate unit tensor space
  ///////////////////////////////////////////////////////////////////////////////////////////////////
  int J0_for_unit_tensors = -1;  // all J0
  int T0_for_unit_tensors = -1;  // all T0
  const bool restrict_positive_N0 = false;  // don't restrict to N0 positive

  // get full set of possible unit tensor labels up to Nmax, N1v truncation
  std::vector<u3shell::RelativeUnitTensorLabelsU3ST> unit_tensor_labels;
  u3shell::GenerateRelativeUnitTensorLabelsU3ST(
      run_parameters.Nmax, run_parameters.N1v,
      unit_tensor_labels,J0_for_unit_tensors,T0_for_unit_tensors,
      restrict_positive_N0
    );


  // generate unit tensor subspaces
  u3shell::RelativeUnitTensorSpaceU3S
    unit_tensor_space(run_parameters.Nmax,run_parameters.N1v,unit_tensor_labels);

  ///////////////////////////////////////////////////////////////////////////////////////////////////
  //  Read in observables
  ///////////////////////////////////////////////////////////////////////////////////////////////////

  std::cout << "Reading observables..." << std::endl;

  // Initialize containers for rmes and their symmetries
  // Stored by hw, then by observable
  std::vector<std::vector<u3shell::RelativeRMEsU3SSubspaces>> observables_relative_rmes(run_parameters.hw_values.size());
  std::vector<std::vector<u3shell::IndexedOperatorLabelsU3S>> observable_symmetries_u3s(run_parameters.num_observables);

  spncci::ReadRelativeObservables(
      run_parameters.Nmax, run_parameters.N1v, run_parameters.hw_values,
      run_parameters.observable_directory,run_parameters.observable_filenames,
      unit_tensor_space, observables_relative_rmes, observable_symmetries_u3s
    );

  // Create observable spaces for each observable including Hamiltonian
  std::cout<<"create observable space"<<std::endl;
  std::vector<u3shell::ObservableSpaceU3S> observable_spaces(run_parameters.num_observables);
  for(int ob_num=0; ob_num<run_parameters.num_observables; ++ob_num)
      observable_spaces[ob_num]=u3shell::ObservableSpaceU3S(observable_symmetries_u3s[ob_num]);

  ////////////////////////////////////////////////////////////////
  // terminate counting only run
  ////////////////////////////////////////////////////////////////
  if (run_parameters.count_only)
    {
      // termination
      results_stream.close();

      std::cout << "End of counting-only run" << std::endl;
      std::exit(EXIT_SUCCESS);
    }

  ///////////////////////////////////////////////////////////////////////////////////////////////
  std::cout<<"setting up lgi unit tensor blocks"<<std::endl;
  // Get list of unit tensor labels between lgi's
  std::vector<u3shell::RelativeUnitTensorLabelsU3ST> lgi_unit_tensor_labels;
  u3shell::GenerateRelativeUnitTensorLabelsU3ST(
      run_parameters.Nsigmamax, run_parameters.N1v,
      lgi_unit_tensor_labels,J0_for_unit_tensors,T0_for_unit_tensors,
      restrict_positive_N0
    );

  //Get look-up table for lgi index in full space.  Used for looking up seed filenames
  // which are index by full space index
  std::cout<<"reading lgi table "<<std::endl;
  std::vector<int> lgi_full_space_index_lookup;
  lgi::ReadLGILookUpTable(lgi_full_space_index_lookup,lgi_families.size());
  /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
  std::cout<<"Starting recurrence and contraction"<<std::endl;
  // Get list of lgi pairs with non-zero matrix elements between them.
  // Restricted to ket<=bra.
  //
  // If doing variance truncation run only generate lgi pairs needed for variance calculation
  //    TODO: Finish.  Need to populate reference and test subspace vectors
  //      For now, just set variance_truncation_run to false
  bool variance_truncation_run=false;
  std::vector<spncci::LGIPair> lgi_pairs;
  if(variance_truncation_run)
    {
      std::vector<int> reference_subspace;
      std::vector<int> test_subspace;
      spncci::GetLGIPairsForRecurrence(lgi_full_space_index_lookup,spncci_space, run_parameters.Nmax,reference_subspace,test_subspace,lgi_pairs);
    }
  // Otherwise, get standard set of lgi pairs
  else
    spncci::GetLGIPairsForRecurrence(lgi_full_space_index_lookup,spncci_space,run_parameters.Nmax,lgi_pairs);


  ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
  // If transforming LGI basis
  ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
  spncci::OperatorBlocks lgi_transformations;
  if(run_parameters.transform_lgi)
    {
      std::cout<<"reading in lgi transformations"<<std::endl;
      std::string lgi_transformations_filename="lgi_transformations.dat";
      spncci::ReadTransformationMatrices(lgi_transformations_filename,lgi_transformations);
    }
  ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  
  std::cout<<"begin parallel region"<<std::endl;
  
  //Declaring shared variables
  int num_files;
  spncci::ObservableHypersectorsByLGIPairTable
    observable_hypersectors_mesh(run_parameters.num_observables);

  mcutils::SteadyTimer timer_recurrence;
  timer_recurrence.Start();

  #pragma omp parallel shared(observable_hypersectors_mesh,num_files)
    {
      // Parallelization is currently set up so that each thread needs at least on lgi pair
      //    TODO:Remove this restriction.
      #pragma omp single
      {
        int num_threads=omp_get_num_threads();
        if(num_threads>lgi_pairs.size())
          {
            std::cout<<"Too many threads.  Only "<<lgi_pairs.size()<<" needed."<<std::endl;
            assert(num_threads<=lgi_pairs.size());
          }

        num_files=num_threads;
      }

      //private coefficient caches--avoids locks and barriers
      u3::UCoefCache u_coef_cache;
      u3::PhiCoefCache phi_coef_cache;


      #pragma omp for schedule(dynamic) nowait
      // For each LGI pair, compute SU(3)xSU(2) reduced many-body matrix elements of unit tensors.
      // Then contract unit tensors with relative matrix elements of observables
      // Write observable hypersectors and hyperblocks to separate file for each LGI pair, each
      // observabel and each hw. 
      //
      // Note: Only observable hypersectors with irrep_family_bra>=irrep_family_ket written to files
      // If diagonal sector, only upper triangle stored.  --Is this still correct?
      for(int i=0; i<lgi_pairs.size(); ++i)
        {
          const spncci::LGIPair& lgi_pair=lgi_pairs[i];
          // mcutils::SteadyTimer timer_pair;
          // timer_pair.Start();
          spncci::ComputeManyBodyRMEs(
              run_parameters,lgi_families,lgi_full_space_index_lookup,
              spncci_space,baby_spncci_space,unit_tensor_space, observable_spaces,
              observables_relative_rmes,k_matrix_cache,kinv_matrix_cache,
              lgi_transformations,u_coef_cache,phi_coef_cache,lgi_pair
            );
          // timer_pair.Stop();
          // int bra,ket;
          // std::tie(bra,ket)=lgi_pair;
          // std::string out=fmt::format("Recurrence for pair ({:3d},{:3d}) took {}",bra,ket,timer_pair.ElapsedTime());
          // std::cout<<out<<std::endl;

        }// end lgi_pair

      //After recurrence completed, dealocate coefficient caches
      std::cout<<"ucoef cache size: "<<u_coef_cache.size()<<std::endl;
      u_coef_cache.clear();
      phi_coef_cache.clear();

    } //end parallel region
  timer_recurrence.Stop();
  std::cout<<"Recurrence: "<<timer_recurrence.ElapsedTime()<<std::endl;
  ////////////////////////////////////////////////////////////////
  // calculation mesh master loop
  ////////////////////////////////////////////////////////////////

  // timing start
  mcutils::SteadyTimer timer_mesh;
  timer_mesh.Start();

  // W coefficient cache -- needed for observable branching
  u3::WCoefCache w_cache;

  // for each hw value, solve eigenproblem and get expectation values
  std::cout << "Calculation mesh master loop..." << std::endl;
  for(int hw_index=0; hw_index<run_parameters.hw_values.size(); ++hw_index)
    {

      // retrieve mesh parameters
      double hw = run_parameters.hw_values[hw_index];

      // results output: log start of individual mesh calculation
      spncci::StartNewSection(results_stream,"RESULTS");
      spncci::WriteCalculationParameters(results_stream,hw);

      ////////////////////////////////////////////////////////////////
      // eigenproblem
      ////////////////////////////////////////////////////////////////
      std::cout<<"Solve eigenproblem..."<<std::endl;

      std::vector<spncci::Vector> eigenvalues(run_parameters.J_values.size());  // eigenvalues by J subspace
      std::vector<spncci::Matrix> eigenvectors(run_parameters.J_values.size());  // eigenvectors by J subspace

      // Construct and diagonalize Hamiltonian, do decompositions
      {
        const int observable_index = 0;  // for Hamiltonian
        for(int subspace_index=0; subspace_index<run_parameters.J_values.size(); ++subspace_index)
          {
            // for eigenproblem
            const HalfInt& J=run_parameters.J_values[subspace_index];
            spncci::Vector& eigenvalues_J = eigenvalues[subspace_index];
            spncci::Matrix& eigenvectors_J = eigenvectors[subspace_index];
            //////////////////////////////////////////////////////////////////
            // NEW BRANCHING
            //////////////////////////////////////////////////////////////////
            HalfInt J00 = run_parameters.observable_J0_values[observable_index];

            const spncci::SpaceSpBasis& spbasis_bra=spaces_spbasis[subspace_index];
            const spncci::SpaceSpBasis& spbasis_ket=spaces_spbasis[subspace_index];

            const u3shell::ObservableSpaceU3S& observable_space=observable_spaces[observable_index];

            mcutils::SteadyTimer timer_hamiltonian;
            timer_hamiltonian.Start();

            std::cout<<"  Constructing Hamiltonian matrix"<<std::endl;

            spncci::OperatorBlock hamiltonian_matrix;
            spncci::ConstructSymmetricOperatorMatrix(
                baby_spncci_space,observable_space,
                J00,spbasis_bra,spbasis_ket,lgi_pairs,
                observable_index, hw_index,
                hamiltonian_matrix
              );
            timer_hamiltonian.Stop();
            std::cout<<fmt::format("    time: {}",timer_hamiltonian.ElapsedTime())<<std::endl;

            std::cout << fmt::format("  Diagonalizing: J={}",J) << std::endl;

            mcutils::SteadyTimer timer_eigensolver;
            timer_eigensolver.Start();
            spncci::SolveHamiltonian(
                hamiltonian_matrix,
                run_parameters.num_eigenvalues,
                run_parameters.eigensolver_num_convergence,  // whatever exactly this is...
                run_parameters.eigensolver_max_iterations,
                run_parameters.eigensolver_tolerance,
                eigenvalues_J,eigenvectors_J
              );
            timer_eigensolver.Stop();
            std::cout<<fmt::format("   time: {}",timer_eigensolver.ElapsedTime())<<std::endl;

            //Testing
            //Eigen matrix cast to float is not a good idea
            int binary_float_precision=8;
            std::string eigv_filename=fmt::format("eigenvector_{:02d}_{:02.1f}.dat",TwiceValue(J),hw);
            spncci::WriteEigenvectors(eigenvectors_J,J,eigv_filename,binary_float_precision);

            //////////////////////////////////////////////////////////////////
          }

        // results output: eigenvalues
        spncci::WriteEigenvalues(results_stream,run_parameters.J_values,eigenvalues,run_parameters.gex);

        // do decompositions
        spncci::GenerateDecompositions(baby_spncci_space,spaces_spbasis,run_parameters, hw,results_stream);

      }// End Hamiltonian section
      
      {
      //////////////////////////////////////////////////////////////
      // calculate observable RMEs
      ////////////////////////////////////////////////////////////////

      std::cout << "Calculate observable results..." << std::endl;
      mcutils::SteadyTimer timer_observables;
      timer_observables.Start();

      // observable_results_matrices:
      //   - vector over observable_index
      //   - vector over sector_index
      //   - matrix over (bra_eigenstate_index,ket_eigenstate_index)
      std::vector<spncci::OperatorBlocks> observable_results_matrices;
      observable_results_matrices.resize(run_parameters.num_observables);

      //Get sectors
      std::vector<std::vector<std::pair<int,int>>> observable_sectors(run_parameters.num_observables);
      for (int observable_index=0; observable_index<run_parameters.num_observables; ++observable_index)
        {
          auto& sectors=observable_sectors[observable_index];
          HalfInt J0 = run_parameters.observable_J0_values[observable_index];
          for(int i=0; i<run_parameters.J_values.size(); ++i)
            for(int j=0; j<run_parameters.J_values.size(); ++j)
              {
                HalfInt Jp=run_parameters.J_values[i];
                HalfInt J=run_parameters.J_values[j];
                if(am::AllowedTriangle(J,J0,Jp))
                  sectors.emplace_back(i,j);
              }
        }


      // std::cout<<"calculate observable results"<<std::endl;
      for (int observable_index=0; observable_index<run_parameters.num_observables; ++observable_index)
        {
          HalfInt J0 = run_parameters.observable_J0_values[observable_index];
          auto&sectors=observable_sectors[observable_index];

          observable_results_matrices[observable_index].resize(sectors.size());

          // std::cout<<"Get corresponding observable space"<<std::endl;
          const u3shell::ObservableSpaceU3S& observable_space=observable_spaces[observable_index];

          for(int sector_index=0; sector_index<sectors.size(); ++sector_index)
            {
              int bra_index,ket_index;
              std::tie(bra_index,ket_index)=sectors[sector_index];

              const spncci::SpaceSpBasis& spbasis_bra=spaces_spbasis[bra_index];
              const spncci::SpaceSpBasis& spbasis_ket=spaces_spbasis[ket_index];

              const HalfInt bra_J=run_parameters.J_values[bra_index];
              const HalfInt ket_J=run_parameters.J_values[ket_index];

              spncci::OperatorBlock observable_block;
              spncci::ConstructSymmetricOperatorMatrix(
                  baby_spncci_space,observable_space,
                  J0,spbasis_bra,spbasis_ket,lgi_pairs,
                  observable_index, hw_index,
                  observable_block
                );


              std::cout<<"calculate observable results"<<std::endl;
              Eigen::MatrixXd& observable_results_matrix = observable_results_matrices[observable_index][sector_index];
              
              observable_results_matrix = eigenvectors[bra_index].transpose()
                * observable_block
                * eigenvectors[ket_index];

              std::cout
                << fmt::format("Observable {} bra_J {} ket_J {}",observable_index,bra_J,ket_J)
                << std::endl;
            }
        }

      // end timing
      timer_observables.Stop();
      std::cout << fmt::format("  (Observables: {})",timer_observables.ElapsedTime()) << std::endl;

      // results output: observables

      spncci::WriteObservables(
        results_stream,run_parameters.J_values,observable_sectors,
        observable_results_matrices,run_parameters.gex
      );

      }//observable

    // ////////////////////////////////////////////////////////////////////////////////////////////////////////////
  }

// timing stop
timer_mesh.Stop();
std::cout << fmt::format("(Mesh master loop: {})",timer_mesh.ElapsedTime()) << std::endl;

results_stream.close();


}
